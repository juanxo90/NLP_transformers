{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e23338",
   "metadata": {},
   "source": [
    "## Creating dataset following the [following example](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/quora_duplicate_questions/create_splits.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f328bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022fdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/quora_original/quora_duplicate_questions.tsv')\n",
    "\n",
    "if not os.path.exists(source_file):\n",
    "    print(\"Download file to\", source_file)\n",
    "    util.http_get('http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv', source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f147a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/quora-IR-dataset_test'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/quora-IR-dataset_test/graph'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/quora-IR-dataset_test/information-retrieval'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/quora-IR-dataset_test/classification'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/quora-IR-dataset_test/duplicate-mining'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069d0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pairwise file\n",
    "sentences = {}\n",
    "duplicates = defaultdict(lambda: defaultdict(bool))\n",
    "rows = []\n",
    "with open(source_file, encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in reader:\n",
    "        id1 = row['qid1']\n",
    "        id2 = row['qid2']\n",
    "        question1 = row['question1'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        question2 = row['question2'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        is_duplicate = row['is_duplicate']\n",
    "\n",
    "        if question1 == \"\" or question2 == \"\":\n",
    "            continue\n",
    "\n",
    "        sentences[id1] = question1\n",
    "        sentences[id2] = question2\n",
    "\n",
    "        rows.append({'qid1': id1, 'qid2': id2, 'question1': question1, \n",
    "                     'question2': question2, 'is_duplicate': is_duplicate})\n",
    "\n",
    "        if is_duplicate == '1':\n",
    "            duplicates[id1][id2] = True\n",
    "            duplicates[id2][id1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff25c377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404288\n",
      "149650\n"
     ]
    }
   ],
   "source": [
    "print(len(rows))\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a54968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Nearly) exact duplicates found: 6328\n"
     ]
    }
   ],
   "source": [
    "# Search for (near) exact duplicates\n",
    "# The original Quora duplicate questions dataset is an incomplete annotation,\n",
    "# i.e., there are several duplicate question pairs which are not marked as duplicates.\n",
    "# These missing annotation can make it difficult to compare approaches.\n",
    "# Here we use a simple approach that searches for near identical questions, that only differ in maybe a stopword\n",
    "# We mark these found question pairs also as duplicate to increase the annotation coverage\n",
    "stopwords = set(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'which', 'while', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n",
    "\n",
    "num_new_duplicates = 0\n",
    "sentences_norm = {}\n",
    "\n",
    "for id, sent in sentences.items():\n",
    "    sent_norm = sent.lower()\n",
    "\n",
    "    #Replace some common paraphrases\n",
    "    sent_norm = sent_norm.replace(\"how do you\", \"how do i\").replace(\"how do we\", \"how do i\")\n",
    "    sent_norm = sent_norm.replace(\"how can we\", \"how can i\").replace(\"how can you\", \"how can i\").replace(\"how can i\", \"how do i\")\n",
    "    sent_norm = sent_norm.replace(\"really true\", \"true\")\n",
    "    sent_norm = sent_norm.replace(\"what are the importance\", \"what is the importance\")\n",
    "    sent_norm = sent_norm.replace(\"what was\", \"what is\")\n",
    "    sent_norm = sent_norm.replace(\"so many\", \"many\")\n",
    "    sent_norm = sent_norm.replace(\"would it take\", \"will it take\")\n",
    "\n",
    "    #Remove any punctuation characters\n",
    "    for c in [\",\", \"!\", \".\", \"?\", \"'\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
    "        sent_norm = sent_norm.replace(c, \" \")\n",
    "\n",
    "    #Remove stop words\n",
    "    tokens = sent_norm.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    sent_norm = \"\".join(tokens)\n",
    "\n",
    "\n",
    "    if sent_norm in sentences_norm:\n",
    "        if not duplicates[id][sentences_norm[sent_norm]]:\n",
    "            num_new_duplicates += 1\n",
    "\n",
    "        duplicates[id][sentences_norm[sent_norm]] = True\n",
    "        duplicates[sentences_norm[sent_norm]][id] = True\n",
    "    else:\n",
    "        sentences_norm[sent_norm] = id\n",
    "\n",
    "\n",
    "print(\"(Nearly) exact duplicates found:\", num_new_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf7e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add transitive closure\n",
      "Add transitive closure\n",
      "Add transitive closure\n"
     ]
    }
   ],
   "source": [
    "#Add transitive closure (if a,b and b,c duplicates => a,c are duplicates)\n",
    "new_entries = True\n",
    "while new_entries:\n",
    "    print(\"Add transitive closure\")\n",
    "    new_entries = False\n",
    "    for a in sentences:\n",
    "        for b in list(duplicates[a]):\n",
    "            for c in list(duplicates[b]):\n",
    "                if a != c and not duplicates[a][c]:\n",
    "                    new_entries = True\n",
    "                    duplicates[a][c] = True\n",
    "                    duplicates[c][a] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a66bc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train sentences: 376755\n",
      "Dev sentences: 53255\n",
      "Test sentences: 107921\n"
     ]
    }
   ],
   "source": [
    "#Distribute rows to train/dev/test split\n",
    "#Ensure that sets contain distinct sentences\n",
    "is_assigned = set()\n",
    "random.shuffle(rows)\n",
    "\n",
    "train_ids = set()\n",
    "dev_ids = set()\n",
    "test_ids = set()\n",
    "\n",
    "counter = 0\n",
    "for row in rows:\n",
    "    if row['qid1'] in is_assigned and row['qid2'] in is_assigned:\n",
    "        continue\n",
    "    elif row['qid1'] in is_assigned or row['qid2'] in is_assigned:\n",
    "\n",
    "        if row['qid2'] in is_assigned: #Ensure that qid1 is assigned and qid2 not yet\n",
    "            row['qid1'], row['qid2'] = row['qid2'], row['qid1']\n",
    "\n",
    "        #Move qid2 to the same split as qid1\n",
    "        target_set = train_ids\n",
    "        if row['qid1'] in dev_ids:\n",
    "            target_set = dev_ids\n",
    "        elif row['qid1'] in test_ids:\n",
    "            target_set = test_ids\n",
    "\n",
    "    else:\n",
    "        #Distribution about 85%/5%/10%\n",
    "        target_set = train_ids\n",
    "        if counter%10 == 0:\n",
    "            target_set = dev_ids\n",
    "        elif counter%10 == 1 or counter%10 == 2:\n",
    "            target_set = test_ids\n",
    "        counter += 1\n",
    "\n",
    "    #Get the sentence with all duplicates and add it to the respective sets\n",
    "    target_set.add(row['qid1'])\n",
    "    is_assigned.add(row['qid1'])\n",
    "\n",
    "    target_set.add(row['qid2'])\n",
    "    is_assigned.add(row['qid2'])\n",
    "\n",
    "    for b in list(duplicates[row['qid1']])+list(duplicates[row['qid2']]):\n",
    "        target_set.add(b)\n",
    "        is_assigned.add(b)\n",
    "\n",
    "\n",
    "#Assert all sets are mutually exclusive\n",
    "assert len(train_ids.intersection(dev_ids)) == 0\n",
    "assert len(train_ids.intersection(test_ids)) == 0\n",
    "assert len(test_ids.intersection(dev_ids)) == 0\n",
    "\n",
    "\n",
    "print(\"\\nTrain sentences:\", len(train_ids))\n",
    "print(\"Dev sentences:\", len(dev_ids))\n",
    "print(\"Test sentences:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d29a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train duplicates 227100\n",
      "Dev duplicates 20840\n",
      "Test duplicates 55265\n"
     ]
    }
   ],
   "source": [
    "#Extract the ids for duplicate questions for train/dev/test\n",
    "def get_duplicate_set(ids_set):\n",
    "    dups_set = set()\n",
    "    for a in ids_set:\n",
    "        for b in duplicates[a]:\n",
    "            ids = sorted([a,b])\n",
    "            dups_set.add(tuple(ids))\n",
    "    return dups_set\n",
    "\n",
    "train_duplicates = get_duplicate_set(train_ids)\n",
    "dev_duplicates = get_duplicate_set(dev_ids)\n",
    "test_duplicates = get_duplicate_set(test_ids)\n",
    "\n",
    "\n",
    "print(\"\\nTrain duplicates\", len(train_duplicates))\n",
    "print(\"Dev duplicates\", len(dev_duplicates))\n",
    "print(\"Test duplicates\", len(test_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0182d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write duplicate graph in pairwise format\n",
      "Write duplicate graph in list format\n",
      "Write duplicate graph in connected subgraph format\n"
     ]
    }
   ],
   "source": [
    "############### Write general files about the duplate questions graph ############\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/graph/sentences.tsv', 'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tquestion\\n\")\n",
    "    for id, question in sentences.items():\n",
    "        fOut.write(\"{}\\t{}\\n\".format(id, question))\n",
    "\n",
    "duplicates_list = set()\n",
    "for a in duplicates:\n",
    "    for b in duplicates[a]:\n",
    "        duplicates_list.add(tuple(sorted([int(a), int(b)])))\n",
    "\n",
    "\n",
    "duplicates_list = list(duplicates_list)\n",
    "duplicates_list = sorted(duplicates_list, key=lambda x: x[0]*1000000+x[1])\n",
    "\n",
    "\n",
    "print(\"\\nWrite duplicate graph in pairwise format\")\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/graph/duplicates-graph-pairwise.tsv', 'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid1\\tqid2\\n\")\n",
    "    for a, b in duplicates_list:\n",
    "        fOut.write(\"{}\\t{}\\n\".format(a, b))\n",
    "\n",
    "\n",
    "print(\"Write duplicate graph in list format\")\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/graph/duplicates-graph-list.tsv', 'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid1\\tqid2\\n\")\n",
    "    for a in sorted(duplicates.keys(), key=lambda x: int(x)):\n",
    "        if len(duplicates[a]) > 0:\n",
    "            fOut.write(\"{}\\t{}\\n\".format(a, \",\".join(sorted(duplicates[a]))))\n",
    "\n",
    "print(\"Write duplicate graph in connected subgraph format\")\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/graph/duplicates-graph-connected-nodes.tsv', 'w', encoding='utf8') as fOut:\n",
    "    written_qids = set()\n",
    "    fOut.write(\"qids\\n\")\n",
    "    for a in sorted(duplicates.keys(), key=lambda x: int(x)):\n",
    "        if a not in written_qids:\n",
    "            ids = set()\n",
    "            ids.add(a)\n",
    "\n",
    "            for b in duplicates[a]:\n",
    "                ids.add(b)\n",
    "\n",
    "            fOut.write(\"{}\\n\".format(\",\".join(sorted(ids, key=lambda x: int(x)))))\n",
    "            for id in ids:\n",
    "                written_qids.add(id)\n",
    "\n",
    "def write_qids(name, ids_list):\n",
    "    with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/graph/'+name+'-questions.tsv', 'w', encoding='utf8') as fOut:\n",
    "        fOut.write(\"qid\\n\")\n",
    "        fOut.write(\"\\n\".join(sorted(ids_list, key=lambda x: int(x))))\n",
    "\n",
    "write_qids('train', train_ids)\n",
    "write_qids('dev', dev_ids)\n",
    "write_qids('test', test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f72a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Retrival Setup\n",
      "Corpus size: 522931\n",
      "Dev queries: 5000\n",
      "Test queries: 10000\n",
      "--DONE--\n"
     ]
    }
   ],
   "source": [
    "####### Output for duplicate mining #######\n",
    "def write_mining_files(name, ids, dups):\n",
    "    with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/duplicate-mining/'+name+'_corpus.tsv', 'w', \n",
    "               encoding='utf8') as fOut:\n",
    "        fOut.write(\"qid\\tquestion\\n\")\n",
    "        for id in ids:\n",
    "            fOut.write(\"{}\\t{}\\n\".format(id, sentences[id]))\n",
    "\n",
    "    with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/duplicate-mining/'+name+'_duplicates.tsv', 'w', encoding='utf8') as fOut:\n",
    "        fOut.write(\"qid1\\tqid2\\n\")\n",
    "        for a, b in dups:\n",
    "            fOut.write(\"{}\\t{}\\n\".format(a, b))\n",
    "\n",
    "\n",
    "write_mining_files('train', train_ids, train_duplicates)\n",
    "write_mining_files('dev', dev_ids, dev_duplicates)\n",
    "write_mining_files('test', test_ids, test_duplicates)\n",
    "\n",
    "\n",
    "###### Classification dataset #####\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/classification/train_pairs.tsv', 'w', encoding='utf8') as fOutTrain, open('/home/juanxo90/Dev/data/quora-IR-dataset_test/classification/dev_pairs.tsv', 'w', encoding='utf8') as fOutDev, open('/home/juanxo90/Dev/data/quora-IR-dataset_test/classification/test_pairs.tsv', 'w', encoding='utf8') as fOutTest:\n",
    "    fOutTrain.write(\"\\t\".join(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate'])+\"\\n\")\n",
    "    fOutDev.write(\"\\t\".join(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate']) + \"\\n\")\n",
    "    fOutTest.write(\"\\t\".join(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate']) + \"\\n\")\n",
    "\n",
    "    for row in rows:\n",
    "        id1 = row['qid1']\n",
    "        id2 = row['qid2']\n",
    "\n",
    "        target = None\n",
    "        if id1 in train_ids and id2 in train_ids:\n",
    "            target = fOutTrain\n",
    "        elif id1 in dev_ids and id2 in dev_ids:\n",
    "            target = fOutDev\n",
    "        elif id1 in test_ids and id2 in test_ids:\n",
    "            target = fOutTest\n",
    "\n",
    "        if target is not None:\n",
    "            target.write(\"\\t\".join([row['qid1'], row['qid2'], sentences[id1], sentences[id2], row['is_duplicate']]))\n",
    "            target.write(\"\\n\")\n",
    "\n",
    "\n",
    "####### Write files for Information Retrieval #####\n",
    "num_dev_queries = 5000\n",
    "num_test_queries = 10000\n",
    "\n",
    "corpus_ids = train_ids.copy()\n",
    "dev_queries = set()\n",
    "test_queries = set()\n",
    "\n",
    "#Create dev queries\n",
    "rnd_dev_ids = sorted(list(dev_ids))\n",
    "random.shuffle(rnd_dev_ids)\n",
    "\n",
    "for a in rnd_dev_ids:\n",
    "    if a not in corpus_ids:\n",
    "        if len(dev_queries) < num_dev_queries and len(duplicates[a]) > 0:\n",
    "            dev_queries.add(a)\n",
    "        else:\n",
    "            corpus_ids.add(a)\n",
    "\n",
    "        for b in duplicates[a]:\n",
    "            if b not in dev_queries:\n",
    "                corpus_ids.add(b)\n",
    "\n",
    "#Create test queries\n",
    "rnd_test_ids = sorted(list(test_ids))\n",
    "random.shuffle(rnd_test_ids)\n",
    "\n",
    "for a in rnd_test_ids:\n",
    "    if a not in corpus_ids:\n",
    "        if len(test_queries) < num_test_queries and len(duplicates[a]) > 0:\n",
    "            test_queries.add(a)\n",
    "        else:\n",
    "            corpus_ids.add(a)\n",
    "\n",
    "        for b in duplicates[a]:\n",
    "            if b not in test_queries:\n",
    "                corpus_ids.add(b)\n",
    "\n",
    "#Write output for information-retrieval\n",
    "print(\"\\nInformation Retrival Setup\")\n",
    "print(\"Corpus size:\", len(corpus_ids))\n",
    "print(\"Dev queries:\", len(dev_queries))\n",
    "print(\"Test queries:\", len(test_queries))\n",
    "\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/information-retrieval/corpus.tsv', 'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tquestion\\n\")\n",
    "    for id in sorted(corpus_ids, key=lambda id: int(id)):\n",
    "        fOut.write(\"{}\\t{}\\n\".format(id, sentences[id]))\n",
    "\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/information-retrieval/dev-queries.tsv', 'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tquestion\\tduplicate_qids\\n\")\n",
    "    for id in sorted(dev_queries, key=lambda id: int(id)):\n",
    "        fOut.write(\"{}\\t{}\\t{}\\n\".format(id, sentences[id], \",\".join(duplicates[id])))\n",
    "\n",
    "with open('/home/juanxo90/Dev/data/quora-IR-dataset_test/information-retrieval/test-queries.tsv', 'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tquestion\\tduplicate_qids\\n\")\n",
    "    for id in sorted(test_queries, key=lambda id: int(id)):\n",
    "        fOut.write(\"{}\\t{}\\t{}\\n\".format(id, sentences[id], \",\".join(duplicates[id])))\n",
    "\n",
    "\n",
    "print(\"--DONE--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ce186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_data/')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "# Getting de data for the correct format\n",
    "# Getting de data for the correct format\n",
    "tableA = pd.read_csv(os.path.join(dataset_path,'tableA.csv'))\n",
    "tableA['id'] = 'a_' + tableA.id.astype(str).str.zfill(4)\n",
    "tableB = pd.read_csv(os.path.join(dataset_path,'tableB.csv'))\n",
    "tableB['id'] = 'b_' + tableB.id.astype(str).str.zfill(4)\n",
    "train = pd.read_csv(os.path.join(dataset_path,'train.csv'))\n",
    "train['ltable_id'] = 'a_' + train.ltable_id.astype(str).str.zfill(4)\n",
    "train['rtable_id'] = 'b_' + train.rtable_id.astype(str).str.zfill(4)\n",
    "test = pd.read_csv(os.path.join(dataset_path,'test.csv'))\n",
    "test['ltable_id'] = 'a_' + test.ltable_id.astype(str).str.zfill(4)\n",
    "test['rtable_id'] = 'b_' + test.rtable_id.astype(str).str.zfill(4)\n",
    "valid = pd.read_csv(os.path.join(dataset_path,'valid.csv'))\n",
    "valid['ltable_id'] = 'a_' + valid.ltable_id.astype(str).str.zfill(4)\n",
    "valid['rtable_id'] = 'b_' + valid.rtable_id.astype(str).str.zfill(4)\n",
    "\n",
    "all_data = (pd.concat([train, test,valid], ignore_index=True)\n",
    "            .merge(tableA, left_on='ltable_id', right_on='id')\n",
    "            .merge(tableB, left_on='rtable_id', right_on='id')\n",
    "           )[['ltable_id', 'rtable_id', 'title_x', 'title_y', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {}\n",
    "duplicates = defaultdict(lambda: defaultdict(bool))\n",
    "rows = []\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    id1 = row['ltable_id']\n",
    "    id2 = row['rtable_id']\n",
    "    question1 = row['title_x'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    question2 = row['title_y'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    is_duplicate = str(row['label'])\n",
    "\n",
    "    if question1 == \"\" or question2 == \"\":\n",
    "        continue\n",
    "\n",
    "    sentences[id1] = question1\n",
    "    sentences[id2] = question2\n",
    "    \n",
    "    rows.append({'qid1': id1, 'qid2': id2, 'question1': question1, \n",
    "             'question2': question2, 'is_duplicate': is_duplicate})\n",
    "    \n",
    "    if is_duplicate == '1':\n",
    "        duplicates[id1][id2] = True\n",
    "        duplicates[id2][id1] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rows))\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c92158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add transitive closure (if a,b and b,c duplicates => a,c are duplicates)\n",
    "new_entries = True\n",
    "while new_entries:\n",
    "    print(\"Add transitive closure\")\n",
    "    new_entries = False\n",
    "    for a in sentences:\n",
    "        for b in list(duplicates[a]):\n",
    "            for c in list(duplicates[b]):\n",
    "                if a != c and not duplicates[a][c]:\n",
    "                    new_entries = True\n",
    "                    duplicates[a][c] = True\n",
    "                    duplicates[c][a] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribute rows to train/dev/test split\n",
    "#Ensure that sets contain distinct sentences\n",
    "is_assigned = set()\n",
    "random.shuffle(rows)\n",
    "\n",
    "train_ids = set()\n",
    "dev_ids = set()\n",
    "test_ids = set()\n",
    "\n",
    "counter = 0\n",
    "for row in rows:\n",
    "    if row['qid1'] in is_assigned and row['qid2'] in is_assigned:\n",
    "        continue\n",
    "    elif row['qid1'] in is_assigned or row['qid2'] in is_assigned:\n",
    "\n",
    "        if row['qid2'] in is_assigned: #Ensure that qid1 is assigned and qid2 not yet\n",
    "            row['qid1'], row['qid2'] = row['qid2'], row['qid1']\n",
    "\n",
    "        #Move qid2 to the same split as qid1\n",
    "        target_set = train_ids\n",
    "        if row['qid1'] in dev_ids:\n",
    "            target_set = dev_ids\n",
    "        elif row['qid1'] in test_ids:\n",
    "            target_set = test_ids\n",
    "\n",
    "    else:\n",
    "        #Distribution about 85%/5%/10%\n",
    "        target_set = train_ids\n",
    "        if counter%10 == 0:\n",
    "            target_set = dev_ids\n",
    "        elif counter%10 == 1 or counter%10 == 2:\n",
    "            target_set = test_ids\n",
    "        counter += 1\n",
    "\n",
    "    #Get the sentence with all duplicates and add it to the respective sets\n",
    "    target_set.add(row['qid1'])\n",
    "    is_assigned.add(row['qid1'])\n",
    "\n",
    "    target_set.add(row['qid2'])\n",
    "    is_assigned.add(row['qid2'])\n",
    "\n",
    "    for b in list(duplicates[row['qid1']])+list(duplicates[row['qid2']]):\n",
    "        target_set.add(b)\n",
    "        is_assigned.add(b)\n",
    "\n",
    "\n",
    "#Assert all sets are mutually exclusive\n",
    "assert len(train_ids.intersection(dev_ids)) == 0\n",
    "assert len(train_ids.intersection(test_ids)) == 0\n",
    "assert len(test_ids.intersection(dev_ids)) == 0\n",
    "\n",
    "print(\"\\nTrain sentences:\", len(train_ids))\n",
    "print(\"Dev sentences:\", len(dev_ids))\n",
    "print(\"Test sentences:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the ids for duplicate questions for train/dev/test\n",
    "def get_duplicate_set(ids_set):\n",
    "    dups_set = set()\n",
    "    for a in ids_set:\n",
    "        for b in duplicates[a]:\n",
    "            ids = sorted([a,b])\n",
    "            dups_set.add(tuple(ids))\n",
    "    return dups_set\n",
    "\n",
    "train_duplicates = get_duplicate_set(train_ids)\n",
    "dev_duplicates = get_duplicate_set(dev_ids)\n",
    "test_duplicates = get_duplicate_set(test_ids)\n",
    "\n",
    "\n",
    "print(\"\\nTrain duplicates\", len(train_duplicates))\n",
    "print(\"Dev duplicates\", len(dev_duplicates))\n",
    "print(\"Test duplicates\", len(test_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc68d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
