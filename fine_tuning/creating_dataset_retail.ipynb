{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0038cba",
   "metadata": {},
   "source": [
    "# Creating dataset for fine tuning in retail data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dbc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from sentence_transformers import util\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de82ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_data/')\n",
    "if not os.path.exists(dataset_path):\n",
    "    util.http_get(url='https://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Amazon-Google/exp_data/tableA.csv',\n",
    "                  path=os.path.join(dataset_path,'tableA.csv'))\n",
    "    util.http_get(url='https://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Amazon-Google/exp_data/tableB.csv',\n",
    "                  path=os.path.join(dataset_path,'tableB.csv'))\n",
    "    util.http_get(url='https://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Amazon-Google/exp_data/test.csv',\n",
    "                  path=os.path.join(dataset_path,'test.csv'))\n",
    "    util.http_get(url='https://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Amazon-Google/exp_data/train.csv',\n",
    "                  path=os.path.join(dataset_path,'train.csv'))\n",
    "    util.http_get(url='https://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Amazon-Google/exp_data/valid.csv',\n",
    "                  path=os.path.join(dataset_path,'valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2100799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting de data for the correct format\n",
    "# Getting de data for the correct format\n",
    "tableA = pd.read_csv(os.path.join(dataset_path,'tableA.csv'))\n",
    "tableA['id'] = 'a_' + tableA.id.astype(str).str.zfill(4)\n",
    "tableB = pd.read_csv(os.path.join(dataset_path,'tableB.csv'))\n",
    "tableB['id'] = 'b_' + tableB.id.astype(str).str.zfill(4)\n",
    "train = pd.read_csv(os.path.join(dataset_path,'train.csv'))\n",
    "train['ltable_id'] = 'a_' + train.ltable_id.astype(str).str.zfill(4)\n",
    "train['rtable_id'] = 'b_' + train.rtable_id.astype(str).str.zfill(4)\n",
    "test = pd.read_csv(os.path.join(dataset_path,'test.csv'))\n",
    "test['ltable_id'] = 'a_' + test.ltable_id.astype(str).str.zfill(4)\n",
    "test['rtable_id'] = 'b_' + test.rtable_id.astype(str).str.zfill(4)\n",
    "valid = pd.read_csv(os.path.join(dataset_path,'valid.csv'))\n",
    "valid['ltable_id'] = 'a_' + valid.ltable_id.astype(str).str.zfill(4)\n",
    "valid['rtable_id'] = 'b_' + valid.rtable_id.astype(str).str.zfill(4)\n",
    "\n",
    "all_data = (pd.concat([train, test,valid], ignore_index=True)\n",
    "            .merge(tableA, left_on='ltable_id', right_on='id')\n",
    "            .merge(tableB, left_on='rtable_id', right_on='id')\n",
    "           )[['id_x', 'id_y', 'title_x', 'title_y', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e914eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating output directories\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_train/graph'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_train/information-retrieval'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_train/classification'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                            'data/retail_train/duplicate-mining'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41110dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {}\n",
    "duplicates = defaultdict(lambda: defaultdict(bool))\n",
    "rows = []\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    id1 = row['id_x']\n",
    "    id2 = row['id_y']\n",
    "    product1 = row['title_x'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    product2 = row['title_y'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    is_duplicate = str(row['label'])\n",
    "\n",
    "    if product1 == \"\" or product2 == \"\":\n",
    "        continue\n",
    "\n",
    "    sentences[id1] = product1\n",
    "    sentences[id2] = product2\n",
    "    \n",
    "    rows.append({'qid1': id1, 'qid2': id2, 'product1': product1, \n",
    "             'product2': product2, 'is_duplicate': is_duplicate})\n",
    "    \n",
    "    if is_duplicate == '1':\n",
    "        duplicates[id1][id2] = True\n",
    "        duplicates[id2][id1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219e5bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11460\n",
      "2161\n"
     ]
    }
   ],
   "source": [
    "print(len(rows))\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598a2421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add transitive closure\n",
      "Add transitive closure\n"
     ]
    }
   ],
   "source": [
    "#Add transitive closure (if a,b and b,c duplicates => a,c are duplicates)\n",
    "new_entries = True\n",
    "while new_entries:\n",
    "    print(\"Add transitive closure\")\n",
    "    new_entries = False\n",
    "    for a in sentences:\n",
    "        for b in list(duplicates[a]):\n",
    "            for c in list(duplicates[b]):\n",
    "                if a != c and not duplicates[a][c]:\n",
    "                    new_entries = True\n",
    "                    duplicates[a][c] = True\n",
    "                    duplicates[c][a] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ab056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train sentences: 2404\n",
      "Dev sentences: 325\n",
      "Test sentences: 716\n"
     ]
    }
   ],
   "source": [
    "#Distribute rows to train/dev/test split\n",
    "#Ensure that sets contain distinct sentences\n",
    "is_assigned = set()\n",
    "random.shuffle(rows)\n",
    "\n",
    "train_ids = set()\n",
    "dev_ids = set()\n",
    "test_ids = set()\n",
    "\n",
    "counter = 0\n",
    "for row in rows:\n",
    "    if row['qid1'] in is_assigned and row['qid2'] in is_assigned:\n",
    "        continue\n",
    "    elif row['qid1'] in is_assigned or row['qid2'] in is_assigned:\n",
    "\n",
    "        if row['qid2'] in is_assigned: #Ensure that qid1 is assigned and qid2 not yet\n",
    "            row['qid1'], row['qid2'] = row['qid2'], row['qid1']\n",
    "\n",
    "        #Move qid2 to the same split as qid1\n",
    "        target_set = train_ids\n",
    "        if row['qid1'] in dev_ids:\n",
    "            target_set = dev_ids\n",
    "        elif row['qid1'] in test_ids:\n",
    "            target_set = test_ids\n",
    "\n",
    "    else:\n",
    "        #Distribution about 85%/5%/10%\n",
    "        target_set = train_ids\n",
    "        if counter%10 == 0:\n",
    "            target_set = dev_ids\n",
    "        elif counter%10 == 1 or counter%10 == 2:\n",
    "            target_set = test_ids\n",
    "        counter += 1\n",
    "\n",
    "    #Get the sentence with all duplicates and add it to the respective sets\n",
    "    target_set.add(row['qid1'])\n",
    "    is_assigned.add(row['qid1'])\n",
    "\n",
    "    target_set.add(row['qid2'])\n",
    "    is_assigned.add(row['qid2'])\n",
    "\n",
    "    for b in list(duplicates[row['qid1']])+list(duplicates[row['qid2']]):\n",
    "        target_set.add(b)\n",
    "        is_assigned.add(b)\n",
    "\n",
    "\n",
    "#Assert all sets are mutually exclusive\n",
    "assert len(train_ids.intersection(dev_ids)) == 0\n",
    "assert len(train_ids.intersection(test_ids)) == 0\n",
    "assert len(test_ids.intersection(dev_ids)) == 0\n",
    "\n",
    "print(\"\\nTrain sentences:\", len(train_ids))\n",
    "print(\"Dev sentences:\", len(dev_ids))\n",
    "print(\"Test sentences:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f677ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train duplicates 984\n",
      "Dev duplicates 118\n",
      "Test duplicates 288\n"
     ]
    }
   ],
   "source": [
    "#Extract the ids for duplicate products for train/dev/test\n",
    "def get_duplicate_set(ids_set):\n",
    "    dups_set = set()\n",
    "    for a in ids_set:\n",
    "        for b in duplicates[a]:\n",
    "            ids = sorted([a,b])\n",
    "            dups_set.add(tuple(ids))\n",
    "    return dups_set\n",
    "\n",
    "train_duplicates = get_duplicate_set(train_ids)\n",
    "dev_duplicates = get_duplicate_set(dev_ids)\n",
    "test_duplicates = get_duplicate_set(test_ids)\n",
    "\n",
    "\n",
    "print(\"\\nTrain duplicates\", len(train_duplicates))\n",
    "print(\"Dev duplicates\", len(dev_duplicates))\n",
    "print(\"Test duplicates\", len(test_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1aede5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Output for duplicate mining #######\n",
    "def write_mining_files(name, ids, dups):\n",
    "    with open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "             'data/retail_train/duplicate-mining/'+name+'_corpus.tsv'), \n",
    "              'w', encoding='utf8') as fOut:\n",
    "        fOut.write(\"qid\\tproduct\\n\")\n",
    "        for id in ids:\n",
    "            fOut.write(\"{}\\t{}\\n\".format(id, sentences[id]))\n",
    "\n",
    "    with open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "             'data/retail_train/duplicate-mining/'+name+'_duplicates.tsv'), \n",
    "              'w', encoding='utf8') as fOut:\n",
    "        fOut.write(\"qid1\\tqid2\\n\")\n",
    "        for a, b in dups:\n",
    "            fOut.write(\"{}\\t{}\\n\".format(a, b))\n",
    "\n",
    "write_mining_files('train', train_ids, train_duplicates)\n",
    "write_mining_files('dev', dev_ids, dev_duplicates)\n",
    "write_mining_files('test', test_ids, test_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d44705",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Classification dataset #####\n",
    "with open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "                       'data/retail_train/classification/train_pairs.tsv'),\n",
    "          'w', encoding='utf8') as fOutTrain, \\\n",
    "     open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "                       'data/retail_train/classification/dev_pairs.tsv'), \n",
    "     'w', encoding='utf8') as fOutDev, \\\n",
    "     open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "                       'data/retail_train/classification/test_pairs.tsv'),\n",
    "     'w', encoding='utf8') as fOutTest:\n",
    "    fOutTrain.write(\"\\t\".join(['qid1', 'qid2', 'product1', 'product2', 'is_duplicate'])+\"\\n\")\n",
    "    fOutDev.write(\"\\t\".join(['qid1', 'qid2', 'product1', 'product2', 'is_duplicate']) + \"\\n\")\n",
    "    fOutTest.write(\"\\t\".join(['qid1', 'qid2', 'product1', 'product2', 'is_duplicate']) + \"\\n\")\n",
    "\n",
    "    for row in rows:\n",
    "        id1 = row['qid1']\n",
    "        id2 = row['qid2']\n",
    "\n",
    "        target = None\n",
    "        if id1 in train_ids and id2 in train_ids:\n",
    "            target = fOutTrain\n",
    "        elif id1 in dev_ids and id2 in dev_ids:\n",
    "            target = fOutDev\n",
    "        elif id1 in test_ids and id2 in test_ids:\n",
    "            target = fOutTest\n",
    "\n",
    "        if target is not None:\n",
    "            target.write(\"\\t\".join([row['qid1'], \n",
    "                                    row['qid2'], \n",
    "                                    sentences[id1], \n",
    "                                    sentences[id2], \n",
    "                                    row['is_duplicate']]))\n",
    "            target.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7ff09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Write files for Information Retrieval #####\n",
    "num_dev_queries = 5000\n",
    "num_test_queries = 10000\n",
    "\n",
    "corpus_ids = train_ids.copy()\n",
    "dev_queries = set()\n",
    "test_queries = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb88d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dev queries\n",
    "rnd_dev_ids = sorted(list(dev_ids))\n",
    "random.shuffle(rnd_dev_ids)\n",
    "\n",
    "for a in rnd_dev_ids:\n",
    "    if a not in corpus_ids:\n",
    "        if len(dev_queries) < num_dev_queries and len(duplicates[a]) > 0:\n",
    "            dev_queries.add(a)\n",
    "        else:\n",
    "            corpus_ids.add(a)\n",
    "\n",
    "        for b in duplicates[a]:\n",
    "            if b not in dev_queries:\n",
    "                corpus_ids.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1451468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test queries\n",
    "rnd_test_ids = sorted(list(test_ids))\n",
    "random.shuffle(rnd_test_ids)\n",
    "\n",
    "for a in rnd_test_ids:\n",
    "    if a not in corpus_ids:\n",
    "        if len(test_queries) < num_test_queries and len(duplicates[a]) > 0:\n",
    "            test_queries.add(a)\n",
    "        else:\n",
    "            corpus_ids.add(a)\n",
    "\n",
    "        for b in duplicates[a]:\n",
    "            if b not in test_queries:\n",
    "                corpus_ids.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a772d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Retrival Setup\n",
      "Corpus size: 3139\n",
      "Dev queries: 87\n",
      "Test queries: 219\n",
      "--DONE--\n"
     ]
    }
   ],
   "source": [
    "#Write output for information-retrieval\n",
    "print(\"\\nInformation Retrival Setup\")\n",
    "print(\"Corpus size:\", len(corpus_ids))\n",
    "print(\"Dev queries:\", len(dev_queries))\n",
    "print(\"Test queries:\", len(test_queries))\n",
    "\n",
    "with open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "                       'data/retail_train/information-retrieval/corpus.tsv'),\n",
    "          'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tproduct\\n\")\n",
    "    for id in sorted(corpus_ids):\n",
    "        fOut.write(\"{}\\t{}\\n\".format(id, sentences[id]))\n",
    "\n",
    "with open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "                       'data/retail_train/information-retrieval/dev-queries.tsv'),\n",
    "          'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tproduct\\tduplicate_qids\\n\")\n",
    "    for id in sorted(dev_queries):\n",
    "        fOut.write(\"{}\\t{}\\t{}\\n\".format(id, sentences[id], \",\".join(duplicates[id])))\n",
    "\n",
    "with open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\n",
    "                       'data/retail_train/information-retrieval/test-queries.tsv'),\n",
    "          'w', encoding='utf8') as fOut:\n",
    "    fOut.write(\"qid\\tproduct\\tduplicate_qids\\n\")\n",
    "    for id in sorted(test_queries):\n",
    "        fOut.write(\"{}\\t{}\\t{}\\n\".format(id, sentences[id], \",\".join(duplicates[id])))\n",
    "\n",
    "\n",
    "print(\"--DONE--\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
