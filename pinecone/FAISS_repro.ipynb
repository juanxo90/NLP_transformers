{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS intro \n",
    "\n",
    "Reproduction of the code in [FAISS tutorial](https://www.pinecone.io/learn/faiss-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  A group of kids is playing in a yard and an ol...   \n",
       "1        2  A group of children is playing in the house an...   \n",
       "2        3  The young boys are playing outdoors and the ma...   \n",
       "3        5  The kids are playing outdoors near a man with ...   \n",
       "4        9  The young boys are playing outdoors and the ma...   \n",
       "\n",
       "                                          sentence_B  relatedness_score  \\\n",
       "0  A group of boys in a yard is playing and a man...                4.5   \n",
       "1  A group of kids is playing in a yard and an ol...                3.2   \n",
       "2  The kids are playing outdoors near a man with ...                4.7   \n",
       "3  A group of kids is playing in a yard and an ol...                3.4   \n",
       "4  A group of kids is playing in a yard and an ol...                3.7   \n",
       "\n",
       "  entailment_judgment  \n",
       "0             NEUTRAL  \n",
       "1             NEUTRAL  \n",
       "2          ENTAILMENT  \n",
       "3             NEUTRAL  \n",
       "4             NEUTRAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')\n",
    "# create dataframe\n",
    "data = pd.read_csv(StringIO(res.text), sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A group of kids is playing in a yard and an old man is standing in the background',\n",
       " 'A group of children is playing in the house and there is no man standing in the background',\n",
       " 'The young boys are playing outdoors and the man is smiling nearby',\n",
       " 'The kids are playing outdoors near a man with a smile',\n",
       " 'The young boys are playing outdoors and the man is smiling nearby']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we take all samples from both sentence A and B\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we take all samples from both sentence A and B\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentence_b = data['sentence_B'].tolist()\n",
    "sentences.extend(sentence_b)  # merge them\n",
    "len(set(sentences))  # together we have ~4.5K unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting more data\n",
    "\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.train.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2013/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/images.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/images.test.tsv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of these dataset have the same structure, so we loop through each creating our sentences data\n",
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    # extract to dataframe\n",
    "    data_1 = pd.read_csv(StringIO(res.text), sep='\\t', header=None, on_bad_lines='skip')\n",
    "    # add to columns 1 and 2 to sentences list\n",
    "    sentences.extend(data_1[1].tolist())\n",
    "    sentences.extend(data_1[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14504"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing nan and duplicated\n",
    "sentences = pd.Series([word for word in list(set(sentences)) if type(word) is str])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14504, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a BERT model using sentence transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# initialize sentence transformer model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "# create sentence embeddings\n",
    "sentence_embeddings = model.encode(sentences.to_list())\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and simple similarity search using IndexFlatL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector size \n",
    "d = sentence_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the index\n",
    "index = faiss.IndexFlatIP(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is the index trained?\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14504"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addeding our embeddings (vectorized text)\n",
    "index.add(sentence_embeddings)\n",
    "# see the total\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching similarities the k nearest neigbors between xq and our data \n",
    "k = 4\n",
    "xq = model.encode([\"Someone sprints with a football\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13300 12426 10782  2979]]\n",
      "CPU times: user 16.7 ms, sys: 1.87 ms, total: 18.6 ms\n",
      "Wall time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5848    A group of football players is running in the ...\n",
       "8755    A group of people playing football is running ...\n",
       "7368            Two groups of people are playing football\n",
       "160     A person playing football is running past an o...\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify our text\n",
    "sentences.iloc[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the numerical vector from faiss\n",
    "# we have 4 vectors to return (k) - so we initialize a zero array to hold them\n",
    "vecs = np.zeros((k, d))\n",
    "# then iterate through each ID from I and add the reconstructed vector to our zero-array\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01627034,  0.22325909, -0.15037404, -0.30747247, -0.27122435,\n",
       "       -0.10593174, -0.06460953,  0.04738246, -0.73349065, -0.37657726,\n",
       "       -0.76762778,  0.16902889,  0.53107685,  0.51176643,  1.14415836,\n",
       "       -0.08562846, -0.67240065, -0.96637088,  0.02545462, -0.2155983 ,\n",
       "       -1.25656593, -0.82982177, -0.09825007, -0.21850885,  0.50610268,\n",
       "        0.10527948,  0.50396878,  0.65242952, -1.39458692,  0.65847504,\n",
       "       -0.21525346, -0.22487436,  0.81818366,  0.08464289, -0.76141709,\n",
       "       -0.28928319, -0.09825802, -0.7304616 ,  0.07855845, -0.84354568,\n",
       "       -0.59242058,  0.77471358, -1.20920539, -0.22757953, -1.30733609,\n",
       "       -0.23081474, -1.31322527,  0.01629097, -0.97285479,  0.19308148,\n",
       "        0.47424552,  1.18920887, -1.96741259, -0.70061088, -0.29638764,\n",
       "        0.60533708,  0.62407476, -0.70340389, -0.86754155,  0.17673104,\n",
       "       -0.19170557, -0.02951936,  0.22623543, -0.1669542 , -0.80402559,\n",
       "       -0.45918953,  0.69675452, -0.24928206, -1.01478684, -0.9217453 ,\n",
       "       -0.33842683, -0.39296773, -0.8373487 , -0.11479253,  0.46049693,\n",
       "       -1.45211208,  0.60310441,  0.38696313, -0.04061206,  0.00453173,\n",
       "        0.24117853,  0.05396278,  0.07506417,  1.05115879,  0.12383965,\n",
       "       -0.71281075,  0.11722887,  0.52238196, -0.04581172,  0.26827082,\n",
       "        0.85985428, -0.3566989 , -0.646671  , -0.54357928, -0.04310531,\n",
       "        0.95139188, -0.15605707, -0.49625334, -0.11140195,  0.15610136])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the index (IVFFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 50  # how many cells\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.train(sentence_embeddings)\n",
    "index.is_trained  # check if index is now trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)\n",
    "index.ntotal  # number of embeddings indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)\n",
    "# FlatL2 result [[ 9884   340 12395  5399]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify our text\n",
    "sentences.iloc[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasin the nprobe \n",
    "index.nprobe = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the vector reconstruction we need to add direct map\n",
    "index.make_direct_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.reconstruct(7460)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "it works in this way\n",
    "1. We split the original vector into several subvectors.\n",
    "2. For each set of subvectors, we perform a clustering operation — creating multiple centroids for each sub-vector set.\n",
    "3. In our vector of sub-vectors, we replace each sub-vector with the ID of it’s nearest set-specific centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the worflow\n",
    "m = 8  # number of centroid IDs in final compressed vectors\n",
    "bits = 8 # number of bits in each centroid\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the index\n",
    "index.train(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our vectors\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the nbrobe\n",
    "index.nprobe = 10  # align to previous IndexIVFFlat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)\n",
    "\n",
    "# FlatL2 result [[ 9884   340 12395  5399]] (15.4 ms) \n",
    "# IVFFlat result nbrobe = 10 [[ 9884   340 12395  5399]] (4.6 ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS selecting right index\n",
    "\n",
    "Index selection will depends of \n",
    "1. Dataset size\n",
    "2. Search Frequency\n",
    "3. Search-Quality vs search-speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat index\n",
    "\n",
    "Is called flat because the vectors are not modified\n",
    "\n",
    "Used\n",
    "* Search quality is a very high priority.\n",
    "* Search time does not matter OR when using a small index (<10K).\n",
    "\n",
    "(See the FlatL2 example above)\n",
    "\n",
    "To improve velocity we need\n",
    "\n",
    "1. Reduce vector size — through dimensionality reduction or reducing the number of bits representing our vectors values.\n",
    "\n",
    "2. Reduce search scope — we can do this by clustering or organizing vectors into tree structures based on certain attributes, similarity, or distance — and restricting our search to closest clusters or filter through most similar branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hassing (LSH)\n",
    "\n",
    "Depending the parameters used we can obtain a range of efficency quality.\n",
    "\n",
    "LSH works by grouping vectors into buckets by processing each vector through a hash function that maximizes hashing collisions — rather than minimizing as is usual with hashing functions.\n",
    "\n",
    "IndexLSH is not suitable if we have large vector dimensionality (128 is already too large). Instead, it is best suited to low-dimensionality vectors — and small indexes.\n",
    "\n",
    "If we find ourselves with large d values or large indexes — we avoid LSH completely, instead focusing on our next index, HNSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH implementation\n",
    "nbits = d*2  # resolution of bucketed vectors\n",
    "# initialize index and add vectors\n",
    "index = faiss.IndexLSH(d, nbits)\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5848 2862 8755  160]]\n",
      "CPU times: user 154 ms, sys: 4.7 ms, total: 159 ms\n",
      "Wall time: 7.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)\n",
    "# FlatL2 result [[5848 8755 7368  160]] (15.4 ms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5848    A group of football players is running in the ...\n",
       "8755    A group of people playing football is running ...\n",
       "7368            Two groups of people are playing football\n",
       "160     A person playing football is running past an o...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FlatL2 result \n",
    "sentences.iloc[[5848, 8755, 7368, 160]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5848    A group of football players is running in the ...\n",
       "2862    A group of football players running down the f...\n",
       "8755    A group of people playing football is running ...\n",
       "160     A person playing football is running past an o...\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify our text\n",
    "sentences.iloc[I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Navigable Small World Graphs (HNSW)\n",
    "\n",
    "HNSW is a further adaption of navigable small world (NSW) graphs — where an NSW graph is a graph structure containing vertices connected by edges to their nearest neighbors.\n",
    "\n",
    "For bigger datasets with higher-dimensionality — HNSW graphs are some of the best performing indexes we can use.\n",
    "\n",
    "So, where RAM is not a limiting factor HNSW is great as a well-balanced index that we can push to focus more towards quality by increasing our three parameters.\n",
    "\n",
    "Some important parameters are:\n",
    "* ```M``` — the number of nearest neighbors that each vertex will connect to.\n",
    "* ```efSearch``` — how many entry points will be explored between layers during the search.\n",
    "* ```efConstruction``` — how many entry points will be explored when building the index.\n",
    "\n",
    "```M``` and ```efSearch``` have a larger impact on search-time — ```efConstruction``` primarily increases index construction time (meaning a slower index.add), but at higher ```M``` values and higher query volume we do see an impact from ```efConstruction``` on search-time too.\n",
    "\n",
    "HNSW indexes take up a significant amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set HNSW index parameters\n",
    "M = 64  # number of connections each vertex will have\n",
    "ef_search = 32  # depth of layers explored during search\n",
    "ef_construction = 64  # depth of layers explored during index construction\n",
    "\n",
    "# initialize index (d == 128)\n",
    "index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_INNER_PRODUCT)\n",
    "# set efConstruction and efSearch parameters\n",
    "index.hnsw.efConstruction = ef_construction\n",
    "index.hnsw.efSearch = ef_search\n",
    "# add data to index\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13300 12426 10782  2979]]\n",
      "CPU times: user 22.7 ms, sys: 1.6 ms, total: 24.3 ms\n",
      "Wall time: 1.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)\n",
    "# FlatL2 result [[5848 8755 7368  160]] (15.4 ms) \n",
    "# LSH result [[5848 2862 8755  160]] (7.2 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[273.63864, 273.54172, 270.8955 , 269.7536 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5848    A group of football players is running in the ...\n",
       "8755    A group of people playing football is running ...\n",
       "7368            Two groups of people are playing football\n",
       "160     A person playing football is running past an o...\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\"Someone sprints with a football\"]\n",
    "sentences.iloc[I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Inverted File Index\n",
    "\n",
    "The Inverted File Index (IVF) index consists of search scope reduction through clustering. It’s a very popular index as it’s easy to use, with high search-quality and reasonable search-speed.\n",
    "\n",
    "Just as with our other indexes, we introduce a query vector xq — this query vector must land within one of our cells, at which point we restrict our search scope to that single cell.\n",
    "\n",
    "But there is a problem if our query vector lands near the edge of a cell — there’s a good chance that its closest other datapoint is contained within a neighboring cell. We call this the edge problem.\n",
    "\n",
    "Now, what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the ```nprobe``` value. With ```nprobe``` we can set the number of cells to search.\n",
    "\n",
    "The parameters:\n",
    "* ```nprobe``` — the number of cells to search\n",
    "* ```nlist``` — the number of cells to create\n",
    "\n",
    "A higher nlist means that we must compare our vector to more centroid vectors — but after selecting the nearest centroid’s cells to search, there will be fewer vectors within each cell. So, increase nlist to prioritize search-speed.\n",
    "\n",
    "As for nprobe, we find the opposite. Increasing nprobe increases the search scope — thus prioritizing search-quality.\n",
    "\n",
    "** See IVFFlat example above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final scope...\n",
    "\n",
    "| Index| Memory (MB) | Query Time (ms) | Recall | Notes\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Flat (L2 or IP) | ~500 | ~18 | 1.0 | Good for small datasets or where query time is irrelevant | \n",
    "| LSH | 20 - 600 | 1.7 - 30 | 0.4 - 0.85 | Best for low dimensional data, or small datasets |\n",
    "| HNSW | 600 - 1600 | 0.6 - 2.1 | 0.5 - 0.95 | Very good for quality, high speed, but large memory usage |\n",
    "| IVF | ~520 | 1 - 9 | 0.7 - 0.95 | Good scalable option. High-quality, at reasonable speed and memory usage |\n",
    "\n",
    "Values displayed are approximated to a test realized by JamesBriggs [Here](https://www.pinecone.io/learn/vector-indexes/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kraken-py-38",
   "language": "python",
   "name": "kraken-py-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
